{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6db4c7e4",
   "metadata": {},
   "source": [
    "# UbiOps / Whylabs\n",
    "This is a cookbook that show cases an example integration between UbiOps and WhyLabs. In this cookbook we will train a model, build it and deploy it to the UbiOps environment, using whylogs to log our data for future monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb349ce",
   "metadata": {},
   "source": [
    "## Creating the model\n",
    "This model is trained on a modified version of the [Used cars dataset](https://www.kaggle.com/valchovalev/car-predictor-usa).\n",
    "\n",
    "This will be a very simplistic model to predict the prices of used cars based on features such as (horsepower, mileage, year) which could be a helpful tool to check if a car is worth the price it is offered at."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41632d3",
   "metadata": {},
   "source": [
    "**First we will install our dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d0651f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "!{sys.executable} -m pip install -U pip\n",
    "!{sys.executable} -m pip install pandas --user\n",
    "!{sys.executable} -m pip install sklearn --user\n",
    "!{sys.executable} -m pip install ubiops --user\n",
    "!{sys.executable} -m pip install whylogs --user # Version needed for lib to work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd47b1cf",
   "metadata": {},
   "source": [
    "## Please fill in the configuration variables needed for this cookbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5e101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set WhyLabs config variables\n",
    "WHYLABS_API_KEY = \"whylabs.apikey\"\n",
    "WHYLABS_DEFAULT_ORG_ID = \"org-1\"\n",
    "WHYLABS_DEFAULT_DATASET_ID = \"model-1\"\n",
    "\n",
    "\n",
    "# Set ubiops config variables\n",
    "API_TOKEN = \"Token ubiopsapitoken\" # Make sure this is in the format \"Token token-code\"\n",
    "PROJECT_NAME = \"blog-post\"\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"WHYLABS_API_KEY\"] = WHYLABS_API_KEY\n",
    "os.environ[\"WHYLABS_DEFAULT_ORG_ID\"] = WHYLABS_DEFAULT_ORG_ID\n",
    "os.environ[\"WHYLABS_DEFAULT_DATASET_ID\"] = WHYLABS_DEFAULT_DATASET_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0fa53f",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "You execute the below cell to see the training code and then run it to see the model being trained and then generating a model file to use in the deployment in our next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b422f711",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from whylogs.app.writers import WhyLabsWriter\n",
    "from whylogs.app import Session\n",
    "from whylogs.app.session import get_or_create_session\n",
    "import pickle\n",
    "\n",
    "# Loading the data \n",
    "data = pd.read_csv(\"model/training_used_cars_data_modified.csv\")\n",
    "\n",
    "#profile data and write to WhyLabs\n",
    "today = datetime.datetime.now()\n",
    "yesterday = today - datetime.timedelta(days=1)\n",
    "\n",
    "writer = WhyLabsWriter(\"\", formats=[],)\n",
    "session = Session(project=\"demo-project\", pipeline=\"pipeline-id\", writers=[writer])\n",
    "with session.logger(dataset_timestamp=yesterday) as ylog:\n",
    "    ylog.log_dataframe(data)\n",
    "\n",
    "# Remove rows that are missing data\n",
    "data.dropna(subset=[\"horsepower\", \"mileage\"], inplace=True)\n",
    "\n",
    "# Get prediction column seperate\n",
    "y = data.price.values\n",
    "x_data = data.drop(['price'], axis = 1)\n",
    "\n",
    "# Split the data for testing\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y, random_state=0)\n",
    "\n",
    "# Create the linear regression and fit it to the training data\n",
    "regr = LinearRegression()\n",
    "regr.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = regr.predict(x_test)\n",
    "\n",
    "# The coefficients\n",
    "print(f'Coefficients: \\n{regr.coef_}')\n",
    "# The mean squared error\n",
    "print(f'Mean squared error: {mean_squared_error(y_test, y_pred)}')\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(f'Coefficient of determination: {r2_score(y_test, y_pred)}')\n",
    "\n",
    "# Save the built model to our dployment folder\n",
    "with open('deployment_folder/model.pkl', 'wb') as f:\n",
    "    pickle.dump(regr, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a627c8e",
   "metadata": {},
   "source": [
    "## Creating UbiOps deployment\n",
    "Now that we have built our AI model and saved it let's create a ubiops deployment to serve requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6993d7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOYMENT_NAME = 'used-cars-model'\n",
    "DEPLOYMENT_VERSION = 'v1'\n",
    "\n",
    "# Import all necessary libraries\n",
    "import shutil\n",
    "import os\n",
    "import ubiops as ubiops\n",
    "\n",
    "client = ubiops.ApiClient(ubiops.Configuration(api_key={'Authorization': API_TOKEN}, \n",
    "                                               host='https://api.ubiops.com/v2.1'))\n",
    "api = ubiops.CoreApi(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7c8e79",
   "metadata": {},
   "source": [
    "**Create the deployment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af6c766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Deployment:\n",
    "\n",
    "    def __init__(self, base_directory, context):\n",
    "        \"\"\"\n",
    "        Initialisation method for the deployment. It can for example be used for loading modules that have to be kept in\n",
    "        memory or setting up connections. Load your external model files (such as pickles or .h5 files) here.\n",
    "        :param str base_directory: absolute path to the directory where the deployment.py file is located\n",
    "        :param dict context: a dictionary containing details of the deployment that might be useful in your code.\n",
    "            It contains the following keys:\n",
    "                - deployment (str): name of the deployment\n",
    "                - version (str): name of the version\n",
    "                - input_type (str): deployment input type, either 'structured' or 'plain'\n",
    "                - output_type (str): deployment output type, either 'structured' or 'plain'\n",
    "                - language (str): programming language the deployment is running\n",
    "                - environment_variables (str): the custom environment variables configured for the deployment.\n",
    "                    You can also access those as normal environment variables via os.environ\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Initialising the model\")\n",
    "        self.wl_session = get_or_create_session()\n",
    "        \n",
    "        model_file_name = \"model.pkl\"\n",
    "        model_file = os.path.join(base_directory, model_file_name)\n",
    "\n",
    "        with open(model_file, 'rb') as file:\n",
    "            self.model = pickle.load(file)\n",
    "\n",
    "    def request(self, data):\n",
    "        \"\"\"\n",
    "        Method for deployment requests, called separately for each individual request.\n",
    "        :param dict/str data: request input data. In case of deployments with structured data, a Python dictionary\n",
    "            with as keys the input fields as defined upon deployment creation via the platform. In case of a deployment\n",
    "            with plain input, it is a string.\n",
    "        :return dict/str: request output. In case of deployments with structured output data, a Python dictionary\n",
    "            with as keys the output fields as defined upon deployment creation via the platform. In case of a deployment\n",
    "            with plain output, it is a string. In this example, a dictionary with the key: output.\n",
    "        \"\"\"\n",
    "        print('Loading data')\n",
    "        X = pd.read_csv(data['data'])\n",
    "\n",
    "        print(\"Prediction being made\")\n",
    "        prediction = self.model.predict(X)\n",
    "        \n",
    "        # Writing the prediction to a csv for further use\n",
    "        print('Writing prediction to csv')\n",
    "        pd.DataFrame(prediction).to_csv('prediction.csv', header = ['target'], index_label= 'index')\n",
    "        \n",
    "        return {\n",
    "            \"prediction\": 'prediction.csv',\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ead6fca",
   "metadata": {},
   "source": [
    "**Deploy to our UbiOps environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0664b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create the deployment\n",
    "deployment_template = ubiops.DeploymentCreate(\n",
    "    name=DEPLOYMENT_NAME,\n",
    "    description='Used cars predictions',\n",
    "    input_type='structured',\n",
    "    output_type='structured',\n",
    "    input_fields=[\n",
    "        ubiops.DeploymentInputFieldCreate(\n",
    "            name='data',\n",
    "            data_type='blob',\n",
    "        ),\n",
    "    ],\n",
    "    output_fields=[\n",
    "        ubiops.DeploymentOutputFieldCreate(\n",
    "            name='prediction',\n",
    "            data_type='blob'\n",
    "        ),\n",
    "    ],\n",
    "    labels={\"demo\": \"whylabs\"}\n",
    ")\n",
    "\n",
    "api.deployments_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    data=deployment_template\n",
    ")\n",
    "\n",
    "# Create the version\n",
    "version_template = ubiops.DeploymentVersionCreate(\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    language='python3.8',\n",
    "    memory_allocation=512,\n",
    "    minimum_instances=0,\n",
    "    maximum_instances=1,\n",
    "    maximum_idle_time=1800 # = 30 minutes\n",
    ")\n",
    "\n",
    "api.deployment_versions_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    data=version_template\n",
    ")\n",
    "\n",
    "# Zip the deployment package\n",
    "shutil.make_archive('deployment_folder', 'zip', '.', 'deployment_folder')\n",
    "\n",
    "# Upload the zipped deployment package\n",
    "file_upload_result = api.revisions_file_upload(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    file='deployment_folder.zip'\n",
    ")\n",
    "\n",
    "ready = False\n",
    "while not ready:   \n",
    "    time.sleep(60)\n",
    "    response = api.deployment_versions_list(project_name=PROJECT_NAME,\n",
    "        deployment_name=DEPLOYMENT_NAME)\n",
    "    statuses = [d.status == 'available' for d in response]\n",
    "    ready = all(statuses)\n",
    "    \n",
    "    print(\"Deployments are NOT ready\")\n",
    "\n",
    "print(\"Deployments are ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febe36ec",
   "metadata": {},
   "source": [
    "## Making requests\n",
    "If the previous steps were successful now we should have a deployment ready to receive requests. You will notice that there is a test file called `production_used_cars_data.csv` which we will use to create a deployment request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f670195",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.read_csv('production_used_cars_data.csv')\n",
    "Y\n",
    "Y = Y.drop(columns='price')\n",
    "Y.to_csv('production_used_cars_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67614801",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('production_used_cars_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2395f934",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'production_used_cars_data.csv'\n",
    "\n",
    "# First upload the data to create a blob\n",
    "blob = api.blobs_create(project_name=PROJECT_NAME, file=file_name)\n",
    "\n",
    "# Make a request using the blob id as input.\n",
    "data = {'data': blob.id}\n",
    "res = api.deployment_version_requests_create(\n",
    "    project_name=PROJECT_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    version=DEPLOYMENT_VERSION,\n",
    "    data=data\n",
    ")\n",
    "\n",
    "# Retrieve the resulting blob\n",
    "res_blob_id = res.result['prediction']\n",
    "res_blob = api.blobs_get(PROJECT_NAME, res_blob_id)\n",
    "result_file_name = 'prediction.csv'\n",
    "\n",
    "# Write it to a file for further examination\n",
    "with open(result_file_name, 'w') as f:\n",
    "    f.write(res_blob.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789e7295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With our predictions made, we can write the inferencing data to WhyLabs and compare it against the training data\n",
    "\n",
    "X = pd.read_csv('production_used_cars_data.csv')\n",
    "Y = pd.read_csv('prediction.csv')\n",
    "combined = X\n",
    "combined['price'] = Y['target']\n",
    "with session.logger() as ylog:\n",
    "    ylog.log_dataframe(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3ce884",
   "metadata": {},
   "source": [
    "# #TODO here show some findings from whylabs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660af2df",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We have now trained a model, used whylabs in the process to gain some insight into our training data, saved the AI model file and used it to create a deployment in our UbiOps environment which is now ready to receive requests and logs each request data to whylabs so you can spot the kind of data coming in and be able to improve on your model in the future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
